---
title: "Assignment 1: Classifying Images of Handwritten Digits"
author: "Anonymous & Anonymous"
date: "12 March 2017"
output: 
  pdf_document:
    fig_caption: yes
header-includes:
  - \usepackage{float}
  - \usepackage{subfig}
  - \newcommand{\argmin}{\operatornamewithlimits{argmin}}
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H!')
```

# Introduction

The goal of this assignment is to get a broader understanding of several algorithms for the classification of handwritten digits. The data set we are using for this purpose contains images of 2707 handwritten digits, each one represented by a vector of 256 elements (i.e. a flattened 16 x 16 matrix) and the corresponding true digit it is supposed to display. For evaluation, the data is split into a training set (1707 images) and a test set (1000 images). In the following, we developed our own classifiers to predict digits with the 256-element vector and tested existing ones, being primarily interested in their accuracy on both the training and the test set.

# Task 1: Analyze Distances Between Images

First it is a good idea to identify which of the digits $d = 0, 1, ..., 9$ are difficult to distinguish from each other. Considering the 256-dimensional space $C_d$, we identified, for each digit:

- the center $c_i$ by summing over all instances and dividing by the amount $n_i$
- the radius $r_i = \max{dist(C_d, c_i)}$, the maximum distance of the points from the center, where $dist(.,.)$ denotes the Euclidean distance measure.

From here, we calculated the distances between the centers $dist_{ij} = dist(c_i, c_j)$ for $i, j = 0, 1, ..., 9$ of the 10 clouds resembling the digits and observe which ones lie close or far from each other in the form of a heatmap:

\begin{figure}[H]%
    \centering
    \includegraphics[width=10cm]{out/1_euclidean_digit-dists.png}%
    \caption{Euclidean distance between centers. Smaller values indicate more proximity in the 256-dimensional space.}%
    \label{fig:euclidean_dist}%
\end{figure}

(Obviously, the diagonal is of zero distance because it is just comparing the same digits with each other.) Based on our observations we noticed that classification should be comparably easy when comparing, for instance, **0** with **1**, **0** with **7**, **1** with **3**, or **6** with **7**. Conversely, we might face difficulties classifying between **3** and **5**, **4** and **9**, or **7** and **9**. It turned out that **0** and **1** are overall rather easy to classify (located more remote in the high-dimensional space), whereas **8** and **9** are more intertwined with other numbers (located more in the center). We therefore expect their accuracy to be worse than for **0** and **1**.

# Task 2: Implement and evaluate the simplest classfier

The distance measure developed before could now be used to classify the digits. To identify to which digit a 256-element sequence $x$ (with true digit $i^{*}$) should be attributed, we calculated the distances between $x$ and all $c_d$, such that we get our predicted digit $\hat{i} = \argmin_d{dist'(x, c_d)}$. Instead of just using the Euclidean distance, $dist'(.,.)$ here denotes various distance measures that were used for comparison: Cityblock, Cosine, Euclidean, L1, L2, and Manhattan.

For each of these measures, the integer vector resulting from checking each sequence from the training set with $c_d$ was then compared to the vector of true digits. This was done for both the training set and the test set:

```{r echo = F}
Measure <- c('cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan')
Train <- c(0.7656707674282367, 0.8605741066198008,0.8635032220269478, 0.7656707674282367,  0.8635032220269478, 0.7656707674282367)
Train <- round(Train, 2)
Test <- c(0.721, 0.799, 0.804, 0.721, 0.804, 0.721)
Test <- round(Test, 2)
accs <- data.frame(Measure, Train, Test)
library(knitr)
kable(accs)
```

Accuracies on both the training set and test set were relatively good. This shows that a distance measure explains quite a lot of the differences between the digits. Interestingly, Cosine, Euclidean, and L2 outperformed the others (with Cosine being very slighlty worse than Euclidean and L2, which had equal scores). This should be due to the nature of how the distance is measured; more specifically, whether the formula for the measure includes squared terms or only non-squared terms. This makes sense, intuitively, because squaring allows for describing a relationship in a second-order polynomial, which pronounces large differences that can aid in differentiating between digits.
Our confidence in working with the Euclidean measure for digit classification was strengthened when looking at the accuracies.

To get a more nuanced view, confusion matrices were created. This was again done with the training set (column b) and the test set (column c):

```{r results='asis', echo=F}
capitalize <- function(x) {
  s <- strsplit(x, " ")[[1]]
  paste(toupper(substring(s, 1,1)), substring(s, 2),
      sep="", collapse=" ")
}

size = "5.5"
for (dist in c('cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan')) {
  cat(paste0('\\begin{figure}[H]%
    \\centering
    \\subfloat[Digit distances]{{\\includegraphics[width=', size, 'cm]{out/1_', dist, '_digit-dists.png} }}%
    \\subfloat[Training confusion matrix]{{\\includegraphics[width=', size, 'cm]{out/2_', dist, '_training_confusion_matrix.png} }}%
    \\subfloat[Test confusion matrix]{{\\includegraphics[width=', size, 'cm]{out/2_', dist, '_test_confusion_matrix.png} }}%
    \\caption{', capitalize(dist), '}%
    \\label{fig:example}%
\\end{figure}
'))
}
```

Confusion matrices assist in understanding where misclassification happens. Importantly, the diagonal matrix mostly contained values larger than .50, which means that for a given digit at least half of the predictions were accurate, which is in line with the accuracy table above. As before there are clear differences between the squared distances and the non-squared distances, especially when it comes to predicting the digit **1**. While all distance measures were really good at classifying **1**s as **1**s (i.e. high sensitivity), the non-squared distance measures often classified other numbers as **1**s too (i.e. low specificity). This pattern was almost not present at all in the squared distance measures (compare columns for digit 1 in confusion matrices).

A typical misclassification that happened was that a **9** was predicted even though the true digit was a **4**. This was already to be expected from exploring the heatmap in Task 1. Curiously, other comparisons which were seen as problematic did not cause as much misclassification.

The best accuracies were achieved for the digits **1**, **3**, **6**, and **7**. Worst classification happened for **2**, **4**, and **5**. Contrarily to our expectations, **8** and **9** were classified relatively well, even though the heatmaps suggested otherwise.

# Task 3: Implement a Bayes Rule classifier

A second approach to the problem is to construct and apply a Bayes Rule classifier by using an adequate binning procedure and calculating and comparing posterior probabilities. 

For simplificity, here we only propose a way to achieve good classification between digits $d' = \{5, 7\}$, again by evaluating the training and test error. Our idea was to make a count of the values in the lower area of the 16 x 16 representative matrix, reasoning that we should find larger values (i.e. more "ink") when a **5** was written down as compared to a **7**. To find out how large the lower area should be, we simulated it for all possible amounts of rows in the matrix and chose the one with the lowest test error.

First, priors needed to be constructed, reflecting the relative amount of occurrences of the two digits in the training set: 

$P(D = 5) = \frac{P(D = 5)}{P(D = 5) + P(D = 7)} = \frac{88}{88 + 166} = 0.346$ , 
$P(D = 7) = \frac{P(D = 7)}{P(D = 5) + P(D = 7)} = \frac{166}{88 + 166} = 0.654$

From there, we divided the full range of possible values for the two digits $x \in \{X_{D = 5}, X_{D = 7}\}$ into 10 equally wide bins, whereas each of the bins was containing a relevant portion of the range. This was established by extracting the smallest and largest value, dividing the full interval into 10 parts, and using the 9 resulting inner boundaries as the separators between the bins. Thus, for both digits, we ended up with binned $x' \in \{1, 2, ..., 10\}$.

It was then possible to obtain the likelihoods $P(x' \mid D = 5)$ and $P(x' \mid D = 7)$, by dividing the number of samples in a corresponding bin by the number of all samples for a given digit. Now we only had to apply Bayes rule, which translates as the following (excluding the normalization constant which can be ignored):

$$ P(D = d' \mid x') \propto P(x' \mid D = d') P(D = d') $$

The most successful classifier was the one which summed the lowest 2 rows of the 16 x 16 matrix. The posterior distributions for both digits were summarized in the following plot:

\begin{figure}[H]%
    \centering
    \includegraphics[width=10cm]{out/3_posteriors_2.png}%
    \caption{Histograms of posterior probabilities for binned $x'$ for digit 5 and 7}%
    \label{fig:posteriors_binned}%
\end{figure}

The posterior distributions indicate that from the 4th bin upwards (starting at around -20.5 in our arbitrary unit space), $x$ should be classified as a **5**, otherwise as a **7**. This achieved an accuracy of 92% on the training set and, rather surprisingly, an even larger accuracy of 94% on the test set. Overall, taking between 1 and 4 lowest rows of the 16 x 16 matrix resulted in a high accuracy:

```{r echo = F, warning=F, fig.height=2.5, fig.width=4, fig.cap="Classification accuracy for amount of lower rows chosen"}
Rows <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16)
Trainingset <- c(0.90551181102362199, 0.92125984251968507, 0.92519685039370081, 0.91338582677165359, 0.89370078740157477, 0.87401574803149606, 0.86614173228346458, 0.87007874015748032, 0.87401574803149606, 0.86614173228346458, 0.86220472440944884, 0.82283464566929132, 0.80314960629921262, 0.77165354330708658, 0.77952755905511806, 0.77952755905511806)
Testset <- c(0.90756302521008403, 0.94117647058823528, 0.9327731092436975, 0.91596638655462181, 0.89915966386554624, 0.84873949579831931, 0.83193277310924374, 0.81512605042016806, 0.82352941176470584, 0.77310924369747902, 0.78151260504201681, 0.72268907563025209, 0.72268907563025209, 0.72268907563025209, 0.68907563025210083, 0.68907563025210083)

df <- data.frame(Rows = rep(Rows, 2), Accuracy = c(Trainingset, Testset), Set = c(rep("Train", 16), rep("Test", 16)))

library(ggplot2)
ggplot(df, aes(x = Rows, y = Accuracy, colour = Set)) + geom_line() + theme_bw()
```



# Task 4: Implement a multi-class perceptron algorithm
Some text:

```{r echo = F, warning=F, fig.height=2.5, fig.width=4, fig.cap="Convergence of perceptron for different learning rates"}
df = data.frame()
df.add = data.frame(acc = c(0.145284124194, 0.185120093732, 0.196250732279, 0.195079086116, 0.193321616872, 0.193321616872, 0.193321616872, 0.19273579379, 0.193907439953, 0.194493263035, 0.193321616872, 0.193321616872, 0.193321616872, 0.193321616872, 0.193907439953, 0.193321616872, 0.193907439953, 0.193321616872, 0.193321616872, 0.193321616872, 0.193321616872, 0.193907439953, 0.193907439953, 0.193907439953, 0.193907439953, 0.193907439953, 0.193907439953, 0.193907439953, 0.193321616872, 0.193907439953, 0.193321616872))
df.add$rate = 1.0
df.add$test_acc = 0.218
df.add$iter = seq(0, by = 100, length.out = nrow(df.add))
df = rbind(df, df.add)
df.add = data.frame(acc = c(0.104276508494, 0.15992970123, 0.181605155243, 0.190392501465, 0.191564147627, 0.193907439953, 0.190978324546, 0.19273579379, 0.193321616872, 0.192149970709, 0.19273579379, 0.193321616872, 0.193907439953, 0.19273579379, 0.19273579379, 0.193321616872, 0.193907439953, 0.193907439953, 0.193907439953, 0.19273579379, 0.193321616872, 0.193321616872, 0.193321616872, 0.193907439953, 0.193321616872, 0.193907439953, 0.193321616872, 0.193321616872, 0.193321616872, 0.194493263035, 0.193907439953))
df.add$rate = 0.5
df.add$test_acc = 0.218
df.add$iter = seq(0, by = 100, length.out = nrow(df.add))
df = rbind(df, df.add)
df.add = data.frame(acc = c(0.0720562390158, 0.603983596954, 0.837727006444, 0.886936145284, 0.908611599297, 0.921499707088, 0.919156414763, 0.920913884007, 0.93848857645, 0.949619214997, 0.949619214997, 0.953719976567, 0.962507322789, 0.974223784417, 0.970123022847, 0.980082015231, 0.984182776801, 0.983011130639, 0.98769771529, 0.994141769186, 0.992384299941, 0.998828353837, 1.0))
df.add$rate = 0.1
df.add$test_acc = 0.88
df.add$iter = seq(0, by = 100, length.out = nrow(df.add))
df = rbind(df, df.add)
df.add = data.frame(acc = c(0.104862331576, 0.714704159344, 0.854130052724, 0.884007029877, 0.903339191564, 0.93028705331, 0.8769771529, 0.937902753368, 0.945518453427, 0.939660222613, 0.956649091974, 0.96309314587, 0.961335676626, 0.966022261277, 0.969537199766, 0.983011130639, 0.982425307557, 0.977738722906, 0.990626830697, 0.994141769186, 0.99589923843, 1.0))
df.add$rate = 0.05
df.add$test_acc = 0.879
df.add$iter = seq(0, by = 100, length.out = nrow(df.add))
df = rbind(df, df.add)
df.add = data.frame(acc = c(0.0433509080258, 0.407732864675, 0.515524311658, 0.736965436438, 0.831282952548, 0.851786760398, 0.87287639133, 0.907439953134, 0.899238429994, 0.888693614528, 0.908025776216, 0.920913884007, 0.929701230228, 0.941417691857, 0.951376684241, 0.959578207381, 0.967779730521, 0.971880492091, 0.97539543058, 0.980082015231, 0.983011130639, 0.98359695372, 0.987111892209, 0.99179847686, 0.994727592267, 0.998828353837, 1.0))
df.add$rate = 0.01
df.add$test_acc = 0.874
df.add$iter = seq(0, by = 100, length.out = nrow(df.add))
df = rbind(df, df.add)

df$rate = as.factor(df$rate)
ggplot(df[df$iter < 2800, ], aes(x = iter, y = acc, colour = rate)) + geom_line() + theme_bw() + 
  labs(x = "Iterations", y = "Accuracy", colour = "Learning rate")
```



# Task 5: Find the best possible model with existing software

Michael Nielsen presents his Neural Networks software in his book "Neural Networks and Deep Learning" (http://neuralnetworksanddeeplearning.com/index.html). We adapted his code to run it over our data set. 

