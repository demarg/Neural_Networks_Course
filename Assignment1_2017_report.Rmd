---
title: "Assignment 1: Classifying Images of Handwritten Digits"
author: "Anonymous & Anonymous"
date: "12 March 2017"
output: 
  pdf_document:
    fig_caption: yes
header-includes:
  - \usepackage{float}
  - \usepackage{subfig}
  - \newcommand{\argmin}{\operatornamewithlimits{argmin}}
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H!')
```

# Introduction

The goal of this assignment is to get a broader understanding of several algorithms for the classification of handwritten digits. The data set we are using for this purpose contains images of 2707 handwritten digits, each one represented by a vector of 256 elements (i.e. a flattened $16 \times 16$ matrix) and the corresponding true digit it is supposed to display. For evaluation, the data is split into a training set (1707 images) and a test set (1000 images). In the following, we developed our own classifiers to predict digits with the 256-element vector and tested existing ones, being primarily interested in their accuracy on both the training and the test set.

# Task 1: Analyze Distances Between Images

First it is a good idea to identify which of the digits $d = 0, 1, ..., 9$ are difficult to distinguish from each other. Considering the 256-dimensional space $C_d$, we identified, for each digit:

- the center $c_i$ by summing over all instances and dividing by the amount $n_i$
- the radius $r_i = \max{dist(C_d, c_i)}$, the maximum distance of the points from the center, where $dist(.,.)$ denotes the Euclidean distance measure.

From here, we calculated the distances between the centers $dist_{ij} = dist(c_i, c_j)$ for $i, j = 0, 1, ..., 9$ of the 10 clouds resembling the digits and observe which ones lie close or far from each other in the form of a heatmap:

\begin{figure}[H]%
    \centering
    \includegraphics[width=10cm]{out/1_euclidean_digit-dists.png}%
    \caption{Euclidean distance between centers. Smaller values indicate more proximity in the 256-dimensional space.}%
    \label{fig:euclidean_dist}%
\end{figure}

(Obviously, the diagonal is of zero distance because it is just comparing the same digits with each other.) Based on our observations we noticed that classification should be comparably easy when comparing, for instance, **0** with **1**, **0** with **7**, **1** with **3**, or **6** with **7**. Conversely, we might face difficulties classifying between **3** and **5**, **4** and **9**, or **7** and **9**. It turned out that **0** and **1** are overall rather easy to classify (located more remote in the high-dimensional space), whereas **8** and **9** are more intertwined with other numbers (located more in the center). We therefore expect their accuracy to be worse than for **0** and **1**.

# Task 2: Implement and evaluate the simplest classfier

The distance measure developed before could now be used to classify the digits. To identify to which digit a 256-element sequence $x$ (with true digit $i^{*}$) should be attributed, we calculated the distances between $x$ and all $c_d$, such that we get our predicted digit $\hat{i} = \argmin_d{dist'(x, c_d)}$. Instead of just using the Euclidean distance, $dist'(.,.)$ here denotes various distance measures that were used for comparison: Cityblock, Cosine, Euclidean, L1, L2, and Manhattan.

For each of these measures, the integer vector resulting from checking each sequence from the training set with $c_d$ was then compared to the vector of true digits. This was done for both the training set and the test set:

```{r echo = F}
Measure <- c('cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan')
Train <- c(0.7656707674282367, 0.8605741066198008,0.8635032220269478, 0.7656707674282367,  0.8635032220269478, 0.7656707674282367)
Train <- round(Train, 2)
Test <- c(0.721, 0.799, 0.804, 0.721, 0.804, 0.721)
Test <- round(Test, 2)
accs <- data.frame(Measure, Train, Test)
library(knitr)
kable(accs)
```

Accuracies on both the training set and test set were relatively good. This shows that a distance measure explains quite a lot of the differences between the digits. Interestingly, Cosine, Euclidean, and L2 outperformed the others (with Cosine being very slightly worse than Euclidean and L2, which had equal scores). This should be due to the nature of how the distance is measured; more specifically, whether the formula for the measure includes squared terms or only non-squared terms. This makes sense, intuitively, because squaring allows for describing a relationship in a second-order polynomial, which pronounces large differences that can aid in differentiating between digits.
Our confidence in working with the Euclidean measure for digit classification was strengthened when looking at the accuracies.

To get a more nuanced view, confusion matrices were created. This was again done with the training set (column b) and the test set (column c):

```{r results='asis', echo=F}
capitalize <- function(x) {
  s <- strsplit(x, " ")[[1]]
  paste(toupper(substring(s, 1,1)), substring(s, 2),
      sep="", collapse=" ")
}

size = "5.5"
for (dist in c('cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan')) {
  cat(paste0('\\begin{figure}[H]%
    \\centering
    \\subfloat[Digit distances]{{\\includegraphics[width=', size, 'cm]{out/1_', dist, '_digit-dists.png} }}%
    \\subfloat[Training confusion matrix]{{\\includegraphics[width=', size, 'cm]{out/2_', dist, '_training_confusion_matrix.png} }}%
    \\subfloat[Test confusion matrix]{{\\includegraphics[width=', size, 'cm]{out/2_', dist, '_test_confusion_matrix.png} }}%
    \\caption{', capitalize(dist), '}%
    \\label{fig:example}%
\\end{figure}
'))
}
```

Confusion matrices assist in understanding where misclassification happens. Importantly, the diagonal matrix mostly contained values larger than .50, which means that for a given digit at least half of the predictions were accurate, which is in line with the accuracy table above. As before there are clear differences between the squared distances and the non-squared distances, especially when it comes to predicting the digit **1**. While all distance measures were really good at classifying **1**s as **1**s (i.e. high sensitivity), the non-squared distance measures often classified other numbers as **1**s too (i.e. low specificity). This pattern was almost not present at all in the squared distance measures (compare columns for digit 1 in confusion matrices).

A typical misclassification that happened was that a **9** was predicted even though the true digit was a **4**. This was already to be expected from exploring the heatmap in Task 1. Curiously, other comparisons which were seen as problematic did not cause as much misclassification.

The best accuracies were achieved for the digits **1**, **3**, **6**, and **7**. Worst classification happened for **2**, **4**, and **5**. Contrarily to our expectations, **8** and **9** were classified relatively well, even though the heatmaps suggested otherwise.

# Task 3: Implement a Bayes Rule classifier

A second approach to the problem is to construct and apply a Bayes Rule classifier by using an adequate binning procedure and calculating and comparing posterior probabilities. 

For simplificity, here we only propose a way to achieve good classification between digits $d' = \{5, 7\}$, again by evaluating the training and test error. Our idea was to make a count of the values in the lower area of the $16 \times 16$ representative matrix, reasoning that we should find larger values (i.e. more "ink") when a **5** was written down as compared to a **7**. To find out how large the lower area should be, we simulated it for all possible amounts of rows in the matrix and chose the one with the lowest test error.

First, priors needed to be constructed, reflecting the relative amount of occurrences of the two digits in the training set: 

$P(D = 5) = \frac{P(D = 5)}{P(D = 5) + P(D = 7)} = \frac{88}{88 + 166} = 0.346$ , 
$P(D = 7) = \frac{P(D = 7)}{P(D = 5) + P(D = 7)} = \frac{166}{88 + 166} = 0.654$

From there, we divided the full range of possible values for the two digits $x \in \{X_{D = 5}, X_{D = 7}\}$ into 10 equally wide bins, whereas each of the bins was containing a relevant portion of the range. This was established by extracting the smallest and largest value, dividing the full interval into 10 parts, and using the 9 resulting inner boundaries as the separators between the bins. Thus, for both digits, we ended up with binned $x' \in \{1, 2, ..., 10\}$.

It was then possible to obtain the likelihoods $P(x' \mid D = 5)$ and $P(x' \mid D = 7)$, by dividing the number of samples in a corresponding bin by the number of all samples for a given digit. Now we only had to apply Bayes rule, which translates as the following (excluding the normalization constant which can be ignored):

$$ P(D = d' \mid x') \propto P(x' \mid D = d') P(D = d') $$

The most successful classifier was the one which summed the lowest 2 rows of the $16 \times 16$ matrix. The posterior distributions for both digits were summarized in the Figure 8:

\begin{figure}[H]%
    \centering
    \includegraphics[width=10cm]{out/3_posteriors_2.png}%
    \caption{Histograms of posterior probabilities for binned $x'$ for digit 5 and 7}%
    \label{fig:posteriors_binned}%
\end{figure}

The posterior distributions indicate that from the 4th bin upwards (starting at around -20.5 in our arbitrary unit space), $x$ should be classified as a **5**, otherwise as a **7**. This achieved an accuracy of 92% on the training set and, rather surprisingly, an even larger accuracy of 94% on the test set. Overall, taking between 1 and 4 lowest rows of the $16 \times 16$ matrix resulted in a high accuracy, as seen in Figure 9.

```{r echo = F, warning=F, fig.height=2.5, fig.width=4, fig.cap="Classification accuracy for amount of lower rows chosen"}
Rows <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16)
Trainingset <- c(0.90551181102362199, 0.92125984251968507, 0.92519685039370081, 0.91338582677165359, 0.89370078740157477, 0.87401574803149606, 0.86614173228346458, 0.87007874015748032, 0.87401574803149606, 0.86614173228346458, 0.86220472440944884, 0.82283464566929132, 0.80314960629921262, 0.77165354330708658, 0.77952755905511806, 0.77952755905511806)
Testset <- c(0.90756302521008403, 0.94117647058823528, 0.9327731092436975, 0.91596638655462181, 0.89915966386554624, 0.84873949579831931, 0.83193277310924374, 0.81512605042016806, 0.82352941176470584, 0.77310924369747902, 0.78151260504201681, 0.72268907563025209, 0.72268907563025209, 0.72268907563025209, 0.68907563025210083, 0.68907563025210083)

df <- data.frame(Rows = rep(Rows, 2), Accuracy = c(Trainingset, Testset), Set = c(rep("Train", 16), rep("Test", 16)))

library(ggplot2)
ggplot(df, aes(x = Rows, y = Accuracy, colour = Set)) + geom_line() + theme_bw()
```


# Task 4: Implement a multi-class perceptron algorithm
Finally, we implemented a single-layer perceptron with 10 nodes, one to signify each digit. Since we knew that the the data are linearly separable, we used a slightly adapted version of the Generalized Perceptron Algorithm (Duda et al.), which runs until perfect accuracy is achieved on the training data. 

Using the update formulas $w = w - x$ and $w = w + x$ for too active and not active enough nodes respectively, resulted in all weights rapidly decreasing, until the activations from any input were too small for Python to store and being treated as 0. At that point, no meaningful classification could occur, as all nodes had equal activations, and the algorithm got stuck.

To fix this issue we introduced a learning rate parameter $\eta$, updating weights by $w = w \pm \eta x$ instead. This is very similar to the Adaline algorithm, which suggests using $w = w + \eta x (d - y)$ on a random data point, where $d$ is the desired output and $y$ the output of the network. The term $x (d - y)$ is approximated in our algorithm by the term $sx$, where nodes with a higher activation than the correct node were *counteracted* using $s = -1$, and the underactivated correct node *boosted* using $s = 1$.

Figure 10 shows the convergence of the network using different learning rates. It suggests that a rate of around 0.05 to 0.1 is optimal, as larger values can lead to the network collapsing and lower ones just converge slower.

```{r echo = F, warning=F, fig.height=2.5, fig.width=4, fig.cap="Convergence of perceptron for different learning rates"}
df = data.frame()
df.add = data.frame(acc = c(0.145284124194, 0.185120093732, 0.196250732279, 0.195079086116, 0.193321616872, 0.193321616872, 0.193321616872, 0.19273579379, 0.193907439953, 0.194493263035, 0.193321616872, 0.193321616872, 0.193321616872, 0.193321616872, 0.193907439953, 0.193321616872, 0.193907439953, 0.193321616872, 0.193321616872, 0.193321616872, 0.193321616872, 0.193907439953, 0.193907439953, 0.193907439953, 0.193907439953, 0.193907439953, 0.193907439953, 0.193907439953, 0.193321616872, 0.193907439953, 0.193321616872))
df.add$rate = 1.0
df.add$test_acc = 0.218
df.add$iter = seq(0, by = 100, length.out = nrow(df.add))
df = rbind(df, df.add)
df.add = data.frame(acc = c(0.104276508494, 0.15992970123, 0.181605155243, 0.190392501465, 0.191564147627, 0.193907439953, 0.190978324546, 0.19273579379, 0.193321616872, 0.192149970709, 0.19273579379, 0.193321616872, 0.193907439953, 0.19273579379, 0.19273579379, 0.193321616872, 0.193907439953, 0.193907439953, 0.193907439953, 0.19273579379, 0.193321616872, 0.193321616872, 0.193321616872, 0.193907439953, 0.193321616872, 0.193907439953, 0.193321616872, 0.193321616872, 0.193321616872, 0.194493263035, 0.193907439953))
df.add$rate = 0.5
df.add$test_acc = 0.218
df.add$iter = seq(0, by = 100, length.out = nrow(df.add))
df = rbind(df, df.add)
df.add = data.frame(acc = c(0.0720562390158, 0.603983596954, 0.837727006444, 0.886936145284, 0.908611599297, 0.921499707088, 0.919156414763, 0.920913884007, 0.93848857645, 0.949619214997, 0.949619214997, 0.953719976567, 0.962507322789, 0.974223784417, 0.970123022847, 0.980082015231, 0.984182776801, 0.983011130639, 0.98769771529, 0.994141769186, 0.992384299941, 0.998828353837, 1.0))
df.add$rate = 0.1
df.add$test_acc = 0.88
df.add$iter = seq(0, by = 100, length.out = nrow(df.add))
df = rbind(df, df.add)
df.add = data.frame(acc = c(0.104862331576, 0.714704159344, 0.854130052724, 0.884007029877, 0.903339191564, 0.93028705331, 0.8769771529, 0.937902753368, 0.945518453427, 0.939660222613, 0.956649091974, 0.96309314587, 0.961335676626, 0.966022261277, 0.969537199766, 0.983011130639, 0.982425307557, 0.977738722906, 0.990626830697, 0.994141769186, 0.99589923843, 1.0))
df.add$rate = 0.05
df.add$test_acc = 0.879
df.add$iter = seq(0, by = 100, length.out = nrow(df.add))
df = rbind(df, df.add)
df.add = data.frame(acc = c(0.0433509080258, 0.407732864675, 0.515524311658, 0.736965436438, 0.831282952548, 0.851786760398, 0.87287639133, 0.907439953134, 0.899238429994, 0.888693614528, 0.908025776216, 0.920913884007, 0.929701230228, 0.941417691857, 0.951376684241, 0.959578207381, 0.967779730521, 0.971880492091, 0.97539543058, 0.980082015231, 0.983011130639, 0.98359695372, 0.987111892209, 0.99179847686, 0.994727592267, 0.998828353837, 1.0))
df.add$rate = 0.01
df.add$test_acc = 0.874
df.add$iter = seq(0, by = 100, length.out = nrow(df.add))
df = rbind(df, df.add)

df$rate = as.factor(df$rate)
ggplot(df[df$iter < 2800, ], aes(x = iter, y = acc, colour = rate)) + geom_line() + theme_bw() + 
  labs(x = "Iterations", y = "Accuracy", colour = "Learning rate")
```

When this network was trained to 100% accuracy on the training set, it achieved around 88% accuracy on the test set.

# Task 5: Find the best possible model with existing software

Michael Nielsen presents his Neural Networks software in his book "Neural Networks and Deep Learning" (http://neuralnetworksanddeeplearning.com/index.html). We adapted his code to run it over our data set, which required us to create our own data wrapper (see code in .zip). Simulations were conducted as follows:

- Systematic part: 

    + a geometric series of learning rates in the interval [0.05, 25.6] was taken and compared for each execution.
    + the algorithm was run over 20 epochs to see how the neural network fitted the data and which accuracy it obtained on the test set.
    
- Adaptive part: 

    + The amount of hidden layers was varied between 1, 2 and 3 hidden layers. Layers were dropped when they did not improve over their smaller counterparts.
    + The amount of neurons was set relatively big and reduced step by step to see whether a simpler network was sufficient to achieve the same accuracy.
    + Different batch sizes (for random sampling from the training set) were checked for the more promising networks.
    + Epochs were increased for the more promising networks. 
    
The rationale behind this approach came from the following rules of thumb and insights when fitting a Neural Network:

- One hidden layer is sufficient for the large majority of problems.
- The optimal size of the hidden layer is usually between the size of the input and size of the output layers.
- if the data is linearly separable, then you theoretically don't need any hidden layers at all (i.e. a relatively simple Neural Network should be sufficient)

The declared winner with 89.0% accuracy was, as expected, a simple Neural Network which only included one hidden layer, condensing the input layer of 256 to just 15 neurons, before predicting the 10-neuron output layer. The learning rate was $\eta = 6.4$. It achieved the winning accuracy at 13 epochs. Below is a plot visualizing the convergence and its comparison to two other learning rates.

```{r fig.height=2.5, fig.width=4, fig.cap="Winning Neural Network test accuracy over 20 epochs"}
epochs <- 1:20
acc <- c(0.626,  0.727,  0.8  ,  0.833,  0.826,  0.87 ,  0.855,  0.854,
        0.873,  0.882,  0.874,  0.839,  0.873,  0.89 ,  0.885,  0.877,
        0.887,  0.885,  0.885,  0.876)

acc_2 <- c(0.24 ,  0.32 ,  0.527,  0.577,  0.616,  0.637,  0.662,  0.685,
        0.703,  0.716,  0.723,  0.732,  0.734,  0.747,  0.747,  0.749,
        0.755,  0.759,  0.759,  0.769)

acc_3 <- c(0.268,  0.228,  0.294,  0.305,  0.282,  0.303,  0.355,  0.335,
        0.302,  0.287,  0.343,  0.305,  0.299,  0.342,  0.319,  0.348,
        0.341,  0.351,  0.384,  0.369)

lr <- c(6.4, 0.4, 25.6)

plot_dat <- data.frame(Epochs = rep(1:20, 3), Accuracy = c(acc, acc_2, acc_3), Rate = as.character(c(rep(6.4, 20), rep(0.4, 20), rep(25.6, 20))))

ggplot(plot_dat, aes(x = Epochs, y = Accuracy, colour = Rate)) + geom_line() + theme_bw() + geom_hline(yintercept = 0.89, colour = "black", linetype = "dotted")
```

Unfortunately networks with more than one hidden layer did not perform better than the simple 1-layer networks. This is probably due to the fact that (1) there is not too much data available and (2) the data is linearly separable.


# Comparisons of Results

The resulting accuracies from the different approaches are summarized in the table below.


```{r include = F}
epochs = 20
learn_rate = c(0.05,   0.1 ,   0.2 ,   0.4 ,   0.8 ,   1.6 ,   3.2 ,   6.4 ,
        12.8 ,  25.6)

### 1 hidden layer

#mini-batch 10, hidden neurons 100
acc1 <- c(0.409,  0.147,  0.121,  0.608,  0.636,  0.793,  0.718,  0.483,
        0.527,  0.268) # probably not enough epochs to fit

#mini-batch 10, hidden neurons 30
acc2 <- c(0.406,  0.632,  0.36 ,  0.712,  0.796,  0.873,  0.872,  0.878,
        0.837,  0.165)

#mini-batch 10, hidden neurons 20
acc3 <- c(0.48 ,  0.579,  0.654,  0.743,  0.75 ,  0.843,  0.86 ,  0.881,
        0.82 ,  0.469)

#mini-batch 10, hidden neurons 15
acc4 <- c(0.471,  0.529,  0.542,  0.769,  0.829,  0.85 ,  0.867,  0.873,
        0.86 ,  0.095)

#mini-batch 10, hidden neurons 10
acc5 <- c(0.393,  0.511,  0.528,  0.637,  0.756,  0.807,  0.822,  0.869,
        0.806,  0.793)


### 2 hidden layers
#mini-batch 10, hidden neurons 30/30
acc7 <- c(0.371,  0.542,  0.224,  0.787,  0.82 ,  0.866,  0.868,  0.872,
        0.111,  0.106)


### 2 hidden layers
#mini-batch 10, hidden neurons 10/10
acc8 <- c( 0.361,  0.384,  0.473,  0.622,  0.744,  0.807,  0.832,  0.85 ,
        0.763,  0.608)

### 3 hidden layers
#mini-batch 10, hidden neurons 5/2/5
acc9 <- c(0.222,  0.224,  0.321,  0.352,  0.4  ,  0.346,  0.577,  0.451,
        0.273,  0.255)

Method <- c("NN: 100 neurons, 1 layer", "NN: 30 neurons, 1 layer", "NN: 20 neurons, 1 layer", "NN: 15 neurons, 1 layer", "NN: 10 neurons, 1 layer", "NN: 30+30 neurons, 2 layers", "NN: 10+10 neurons, 2 layers", "NN: 5+2+5 neurons, 3 layers")

max(acc9)

Best_rate <- c(1.6, 6.4, 6.4, 6.4, 6.4, 6.4, 6.4, 3.2)
Accuracy <- c(0.793, 0.878, 0.881, 0.890, 0.869, 0.872, 0.850, 0.577) 

data.frame(Method, Best_rate, Accuracy)


```
